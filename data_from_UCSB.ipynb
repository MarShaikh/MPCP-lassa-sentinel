{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d271c109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Storage required by the CHIRPS zip files: 11.43 GB\n",
      "The above storage is taken up by compressed files, for a better estimate, we use the conversion factor of 12.735(obtained from downloading one file)\n",
      "Total true storage requirement for the CHIRPS dataset: 145.54 GB\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_table_from_link(url: str, class_: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract table data from a web page by scraping elements with a specific CSS class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL of the web page to scrape.\n",
    "    class_ : str\n",
    "        The CSS class name to search for within table cells.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of BeautifulSoup Tag objects containing the matched table cells.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    This function assumes the target table has an id=\"list\" attribute.\n",
    "    It searches for <td> elements within that table matching the specified class.\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    table_ = soup.find(id = \"list\")\n",
    "    list_ = table_.find_all(\"td\", class_=class_)\n",
    "    return list_\n",
    "\n",
    "def find_data_storage(url: str, pattern: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate total storage requirements from size data scraped from a web page.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL of the web page containing size information.\n",
    "    pattern : str\n",
    "        Regex pattern parameter (currently unused - function uses hardcoded pattern).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Total storage size converted to megabytes (MB).\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The function searches for table cells with class=\"size\", extracts numeric values\n",
    "    from text matching the pattern numbers with decimal points, and\n",
    "    sums them. The conversion factor 0.001024 is applied, suggesting conversion\n",
    "    from KiB to MB using binary conversion (1024 bytes per KiB, then /1000).\n",
    "    \"\"\"\n",
    "    storage_list = get_table_from_link(url, class_=\"size\")\n",
    "    \n",
    "    total_storage = 0\n",
    "    for itr in storage_list:\n",
    "        pattern = re.compile(pattern)\n",
    "        if pattern.match(itr.text):\n",
    "            storage_per_file = float(itr.text.split(\" \")[0])\n",
    "            total_storage += storage_per_file\n",
    "\n",
    "    return total_storage * 0.001024 # converting to MB\n",
    "\n",
    "\n",
    "def find_tiff_url(url: str, pattern: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract and construct URLs matching a specified pattern from a web page.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The base URL of the web page to scrape.\n",
    "    pattern : str\n",
    "        Regex pattern to match against href attributes in links.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of complete URLs constructed by combining the base URL\n",
    "        with matching href values.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The function searches for table cells with class=\"link\", extracts href\n",
    "    attributes from anchor tags within those cells, and filters them using\n",
    "    the provided regex pattern. Complete URLs are formed by concatenating\n",
    "    the base URL with the matching href values.\n",
    "    \n",
    "    Assumes each link cell contains at least one anchor tag with an href attribute.\n",
    "    \"\"\"\n",
    "    links = get_table_from_link(url, class_ = \"link\")\n",
    "\n",
    "    all_url = []\n",
    "    for link in links:\n",
    "        temp_url = link.find_all(href = True)[0]['href']\n",
    "        pattern = re.compile(pattern)\n",
    "        if pattern.match(temp_url):\n",
    "            all_url.append(url + temp_url)\n",
    "\n",
    "    return all_url\n",
    "\n",
    "url = \"https://data.chc.ucsb.edu/products/CHIRPS-2.0/africa_daily/tifs/p05/\"\n",
    "year_urls = find_tiff_url(url, pattern = r\"\\d{4}\\/\")\n",
    "\n",
    "# get links to all TIFF files\n",
    "# for year in year_urls:\n",
    "#     data_urls = find_tiff_url(year, pattern = r\"chirps-.*\")\n",
    "#     print(data_urls)\n",
    "\n",
    "\n",
    "# get storage requirements for all tiff files\n",
    "total_storage = 0\n",
    "for year in year_urls:\n",
    "    # the storage output from this function is already in MB\n",
    "    total_storage += find_data_storage(url = year, pattern = r\"\\d+\\..+\")\n",
    "\n",
    "total_storage = total_storage * 0.001 # converting to GB\n",
    "print(f\"Total Storage required by the CHIRPS zip files: {total_storage:.2f} GB\")\n",
    "\n",
    "print(\"The above storage is taken up by compressed files, for a better estimate, we use the conversion factor of 12.735(obtained from downloading one file)\")\n",
    "print(f\"Total true storage requirement for the CHIRPS dataset: {(total_storage * 12.735):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get links to all TIFF files\n",
    "# data_urls is a list of a list with 45 years worth of data from 1981-2025\n",
    "# where index 0 has all data for 1981 and index 1 has 1982 ... index 44 has 2025\n",
    "data_urls = []\n",
    "for year in year_urls:\n",
    "    data_urls.append(find_tiff_url(year, pattern = r\"chirps-.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162ae39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://data.chc.ucsb.edu/products/CHIRPS-2.0/africa_daily/tifs/p05/1981/chirps-v2.0.1981.01.01.tif.gz'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_urls[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip file\n",
    "import gzip\n",
    "\n",
    "def unzip_file(url: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Opens an object at a given url, and returns a decompressed byte object\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    url : str\n",
    "        The base url to the source file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bytes\n",
    "        Decompressed byte object\n",
    "    \"\"\"\n",
    "    unzipped_file = requests.get(url) \n",
    "    if unzipped_file.status_code == 200:\n",
    "        decompressed_file = gzip.decompress(unzipped_file.content)\n",
    "    \n",
    "    return decompressed_file\n",
    "\n",
    "url = data_urls[0][1]\n",
    "y = \"1981/\"\n",
    "\n",
    "# getting file name from url\n",
    "file_name = url.split(y)[1].replace(\".gz\", \"\")\n",
    "\n",
    "# I don't need to save this, I could just as well pass a decompressed byte object to downstream tasks\n",
    "decompressed_file = unzip_file(url)\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(decompressed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a79e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipping tiff to Nigeria specific bounding box\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.warp import transform_bounds\n",
    "\n",
    "def clip_to_cog(input_tiff: str, clipped_tiff: str, bbox: list, bbox_crs: str):\n",
    "    \"\"\"\n",
    "    Clips a GeoTIFF to a specified bounding box, handling differing CRS,\n",
    "    and saves it as a Cloud-Optimized GeoTIFF (COG).\n",
    "\n",
    "    Args:\n",
    "        input_tiff: Path to the source GeoTIFF file.\n",
    "        clipped_tiff: Path for the output clipped COG file.\n",
    "        bbox: A list representing the bounding box in the format\n",
    "              [min_x, min_y, max_x, max_y].\n",
    "        bbox_crs: The Coordinate Reference System of the provided bounding box,\n",
    "                  defaulting to WGS84 ('EPSG:4326').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(input_tiff) as src:\n",
    "        \n",
    "            # Get the CRS of the source raster\n",
    "            src_crs = src.crs\n",
    "            \n",
    "            # Reproject the bounding box if the CRS are different\n",
    "            if CRS.from_string(bbox_crs) != src_crs:\n",
    "                left, bottom, right, top = transform_bounds(\n",
    "                    CRS.from_string(bbox_crs),\n",
    "                    src_crs,\n",
    "                    *bbox\n",
    "                )\n",
    "                reprojected_bbox = [left, bottom, right, top]\n",
    "            else:\n",
    "                reprojected_bbox = bbox\n",
    "        \n",
    "        \n",
    "            window = from_bounds(*reprojected_bbox, src.transform)\n",
    "            data = src.read(window=window)\n",
    "            window_transform = src.window_transform(window)\n",
    "\n",
    "            profile = src.profile.copy()\n",
    "            profile.update({\n",
    "                'height': window.height, \n",
    "                'width': window.width, \n",
    "                'transform': window_transform,\n",
    "                'tiled': True, \n",
    "                'blockxsize': 512, \n",
    "                'blockysize': 512,\n",
    "                'compress': 'deflate'\n",
    "            })\n",
    "\n",
    "            # write COG\n",
    "            with rasterio.open(clipped_tiff, 'w', **profile) as dst:\n",
    "                dst.write(data)\n",
    "\n",
    "                factors =  [2, 4, 8, 16]\n",
    "                dst.build_overviews(factors, Resampling.average)\n",
    "                dst.update_tags(ns='rio_overview', resampling='average')\n",
    "    except Exception as e:\n",
    "        print(f\"An error has occurred {e}\")\n",
    "\n",
    "\n",
    "input_tiff = \"chirps-v2.0.1981.01.01.tif\"\n",
    "clipped_tiff = f\"nigeria-cog-{input_tiff}\"\n",
    "bbox_aoi = [2.316388, 3.837669, 15.126447, 14.153350]\n",
    "bbox_crs = \"EPSG:4326\"\n",
    "\n",
    "clip_to_cog(input_tiff, clipped_tiff, bbox_aoi, bbox_crs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPCP_lassasentinel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
